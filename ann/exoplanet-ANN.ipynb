{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import math\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 10, 6\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import recall_score, classification_report, precision_score, confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "from scipy import ndimage\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('/Users/nageshsinghchauhan/Downloads/ML/kaggle/exoplanet/exoTest.csv').fillna(0)\n",
    "train_data = pd.read_csv('/Users/nageshsinghchauhan/Downloads/ML/kaggle/exoplanet/exoTrain.csv').fillna(0)\n",
    "\n",
    "categ = {2: 1,1: 0}\n",
    "train_data.LABEL = [categ[item] for item in train_data.LABEL]\n",
    "test_data.LABEL = [categ[item] for item in test_data.LABEL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reduce memory\n",
    "def reduce_memory(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "test_data = reduce_memory(test_data)\n",
    "train_data = reduce_memory(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualization:\n",
    "plt.figure(figsize=(6,4))\n",
    "colors = [\"0\", \"1\"]\n",
    "sns.countplot('LABEL', data=train_data, palette=colors)\n",
    "plt.title('Class Distributions \\n (0: Not Exoplanet || 1: Exoplanet)', fontsize=14)\n",
    "\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 13, 8\n",
    "plt.title('Distribution of flux values', fontsize=10)\n",
    "plt.xlabel('Flux values')\n",
    "plt.ylabel('Flux intensity')\n",
    "plt.plot(train_data.iloc[0,])\n",
    "plt.plot(train_data.iloc[1,])\n",
    "plt.plot(train_data.iloc[2,])\n",
    "plt.plot(train_data.iloc[3,])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gaussian histogram of no exoplanets\n",
    "labels_1=[100,200,300]\n",
    "for i in labels_1:\n",
    "    plt.hist(train_data.iloc[i,:], bins=200)\n",
    "    plt.title(\"Gaussian Histogram\")\n",
    "    plt.xlabel(\"Flux values\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gaussian histogram of exoplanets\n",
    "labels_1=[16,21,25]\n",
    "for i in labels_1:\n",
    "    plt.hist(train_data.iloc[i,:], bins=200)\n",
    "    plt.title(\"Gaussian Histogram\")\n",
    "    plt.xlabel(\"Flux values\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the data\n",
    "x_train = train_data.drop([\"LABEL\"],axis=1)\n",
    "y_train = train_data[\"LABEL\"]   \n",
    "x_test = test_data.drop([\"LABEL\"],axis=1)\n",
    "y_test = test_data[\"LABEL\"]\n",
    "\n",
    "#Normalizing the data\n",
    "x_train = normalized = normalize(x_train)\n",
    "x_test = normalize(x_test)\n",
    "\n",
    "#Applying of gaussian filter\n",
    "x_train = filtered = ndimage.filters.gaussian_filter(x_train, sigma=10)\n",
    "x_test = ndimage.filters.gaussian_filter(x_test, sigma=10)\n",
    "\n",
    "#Feature scaling\n",
    "std_scaler = StandardScaler()\n",
    "x_train = scaled = std_scaler.fit_transform(x_train)\n",
    "x_test = std_scaler.fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dimentioanlity reduction\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA() \n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_test = pca.transform(X_test)\n",
    "total=sum(pca.explained_variance_)\n",
    "k=0\n",
    "current_variance=0\n",
    "while current_variance/total < 0.90:\n",
    "    current_variance += pca.explained_variance_[k]\n",
    "    k=k+1\n",
    "\n",
    "\"\"\"\n",
    "This plot tells us that selecting 35 components we can preserve something around 98.8% or 99% \n",
    "of the total variance of the data. It makes sense, weâ€™ll not use 100% of our variance,\n",
    "because it denotes all components, and we want only the principal ones.\n",
    "\"\"\"\n",
    "\n",
    "#Apply PCA with n_componenets\n",
    "pca = PCA(n_components=37)\n",
    "x_train = pca.fit_transform(x_train)\n",
    "x_test = pca.transform(x_test)\n",
    "plt.figure()\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Variance (%)') #for each component\n",
    "plt.title('Exoplanet Dataset Explained Variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_records(x_train)     \n",
    "corr = df.corr(method='kendall')\n",
    "plt.figure(figsize=(15,8))\n",
    "sns.heatmap(corr, annot=True)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resampling as the data is highly unbalanced.\n",
    "print(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train==1)))\n",
    "print(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train==0)))\n",
    "\n",
    "sm = SMOTE(random_state=27, ratio = 1.0)\n",
    "x_train_res, y_train_res = sm.fit_sample(x_train, y_train.ravel()) \n",
    "\n",
    "print(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_res==1)))\n",
    "print(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_res==0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Artificial Neural Network\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(101)\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.models import Sequential # initialize neural network library\n",
    "from keras.layers import Dense # build our layers library\n",
    "def build_classifier():\n",
    "    classifier = Sequential() # initialize neural network\n",
    "    classifier.add(Dense(units = 4, kernel_initializer = 'uniform', activation = 'relu', input_dim = x_train_res.shape[1]))\n",
    "    classifier.add(Dense(units = 4, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "    classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
    "    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    return classifier\n",
    "\n",
    "classifier = KerasClassifier(build_fn = build_classifier, epochs = 40)\n",
    "accuracies = cross_val_score(estimator = classifier, X = x_train_res, y = y_train_res, cv = 5, n_jobs = -1)\n",
    "mean = accuracies.mean()\n",
    "variance = accuracies.std()\n",
    "print(\"Accuracy mean: \"+ str(mean))\n",
    "print(\"Accuracy variance: \"+ str(variance))\n",
    "\n",
    "#Accuracy mean: 0.7791307791307792\n",
    "#Accuracy variance: 0.26064381617895693"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
